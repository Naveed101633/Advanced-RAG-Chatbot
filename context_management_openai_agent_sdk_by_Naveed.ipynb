{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4J/uNFOkxP+3d/Vk30sxA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Naveed101633/Advanced-RAG-Chatbot/blob/main/context_management_openai_agent_sdk_by_Naveed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Install openai-agents SDK*"
      ],
      "metadata": {
        "id": "1Sff-FRn882F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veCAI5w9ZU9v",
        "outputId": "1b39f6d0-1b25-445e-9fef-9036df5ba226"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/121.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -Uq openai-agents"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Agent Configuration*"
      ],
      "metadata": {
        "id": "CF0DANW59F7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "JZbYEuPuaqjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel\n",
        "from agents import (\n",
        "    AsyncOpenAI,\n",
        "    OpenAIChatCompletionsModel,\n",
        "    RunConfig\n",
        ")\n",
        "\n",
        "from google.colab import userdata\n",
        "gemini_api_key = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "external_client = AsyncOpenAI(\n",
        "    api_key= gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        ")\n",
        "\n",
        "model = OpenAIChatCompletionsModel(\n",
        "  model= 'gemini-2.0-flash',\n",
        "  openai_client= external_client\n",
        ")\n",
        "\n",
        "from agents import set_default_openai_client, set_tracing_disabled\n",
        "set_tracing_disabled(True)\n",
        "set_default_openai_client(external_client)"
      ],
      "metadata": {
        "id": "DzwDubc9a11b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "563eDel3ht_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Local Context Use Case 1: Student Scenario*"
      ],
      "metadata": {
        "id": "_HInKacs9a1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from dataclasses import dataclass\n",
        "from agents import Agent, Runner, function_tool, RunContextWrapper\n",
        "from openai.types.responses import ResponseTextDeltaEvent\n",
        "\n",
        "@dataclass\n",
        "class Student:\n",
        "    name: str\n",
        "    department: str\n",
        "    roll_no: str\n",
        "    cgpa: str\n",
        "\n",
        "@function_tool\n",
        "async def fetch_student_name(wrapper: RunContextWrapper[Student]) -> str:\n",
        "    \"\"\"Returns the student's name.\"\"\"\n",
        "    return wrapper.context.name\n",
        "\n",
        "@function_tool\n",
        "async def fetch_student_department(wrapper: RunContextWrapper[Student]) -> str:\n",
        "    \"\"\"Returns the student's department.\"\"\"\n",
        "    return f\"{wrapper.context.name} is enrolled in {wrapper.context.department}.\"\n",
        "\n",
        "@function_tool\n",
        "async def fetch_student_cgpa(wrapper: RunContextWrapper[Student]) -> str:\n",
        "    \"\"\"Returns the student's CGPA as a string.\"\"\"\n",
        "    return f\"{wrapper.context.cgpa}\"\n",
        "\n",
        "async def main():\n",
        "    # Create student context\n",
        "    student_info = Student(name=\"Naveed Ahmed\", department=\"IT\", roll_no=94, cgpa=3.45)\n",
        "\n",
        "    # Create agent with type hint\n",
        "    agent = Agent[Student](\n",
        "        name=\"Assistant\",\n",
        "        instructions=\"You are a helpful assistant that provides student information using the provided tools.\",\n",
        "        tools=[fetch_student_name, fetch_student_department, fetch_student_cgpa],\n",
        "        model=model  # Assumes model is defined (e.g., gemini-2.0-flash)\n",
        "    )\n",
        "\n",
        "\n",
        "    result = await Runner.run(\n",
        "        starting_agent= agent,\n",
        "        input= \"What is the student's name and CGPA?\",\n",
        "        context= student_info\n",
        "    )\n",
        "\n",
        "    print(result.final_output)\n",
        "\n",
        "# Run the program\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNDY5p_ciAt6",
        "outputId": "3476c99d-a157-43ed-a430-73859c7474bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The student's name is Naveed Ahmed and their CGPA is 3.45.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Dynamic Instructioms with context*"
      ],
      "metadata": {
        "id": "0WkX1psKzBFX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Local Context Use Case 12: Poets*"
      ],
      "metadata": {
        "id": "tkuFc4k-9wdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import random\n",
        "from typing import Literal\n",
        "from agents import Agent, Runner, RunContextWrapper\n",
        "\n",
        "class Mehfil():\n",
        "  def __init__(self, style: Literal[\"Latif\",\"Jaun\", \"Iqbal\"]):\n",
        "    self.style = style\n",
        "\n",
        "\n",
        "def Legend_Poets(\n",
        "  wrapper: RunContextWrapper[Mehfil], agent: Agent[Mehfil]\n",
        ") -> str:\n",
        "  context = wrapper.context\n",
        "  if context.style ==  \"Latif\":\n",
        "    return \"Respond like Shah Abdul Latif Bhittai\"\n",
        "  elif context.style == \"Jaun\":\n",
        "    return \"Respond like the poet Jaun Elia\"\n",
        "  elif context.style == \"Iqbal\":\n",
        "    return \"Respond like Allama Iqbal\"\n",
        "  else:\n",
        "    return \"Respond like a robot\"\n",
        "\n",
        "\n",
        "agent = Agent(\n",
        "    name= \"Poets\",\n",
        "    instructions= Legend_Poets,\n",
        "    model= model\n",
        ")\n",
        "\n",
        "\n",
        "async def main():\n",
        "  choice= random.choice([\"Latif\", \"Jaun\", \"Iqbal\"])\n",
        "  context = Mehfil(style= choice)\n",
        "  print(f\"Using style {choice} \\n\")\n",
        "\n",
        "  user_message = \"Tell me Breakup poetry in roman urdu\"\n",
        "  print(f\"User Message {user_message}\")\n",
        "\n",
        "  result = await Runner.run(agent, user_message, context= context)\n",
        "  print(f\"Assistant: {result.final_output}\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14yBcH3fCGUv",
        "outputId": "22d975b1-ab4d-47dc-a7fb-eddeb16d9504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using style Jaun \n",
            "\n",
            "User Message Tell me Breakup poetry in roman urdu\n",
            "Assistant: Aisay bichra hai woh, jaise koi khwab tha,\n",
            "Aankh khuli toh bas, ek azaab tha.\n",
            "\n",
            "Tanhaiyon ki ye nagri, ab mera sheher hai,\n",
            "Dil toh jaise koi, toot'ta hua shajar hai.\n",
            "\n",
            "Woh kehta tha sitare tod launga,\n",
            "Magar dekho, aasman bhi toh be-asar hai.\n",
            "\n",
            "Muhabbat ki kitaab toh, jal gayi hogi,\n",
            "Kagaz hi kya bachega, dil bhi agar hai?\n",
            "\n",
            "Ab Jaun, ye rona dhona chhod bhi de,\n",
            "Woh shakhs toh kisi aur ka humsafar hai.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New *Agent/LLM context*"
      ],
      "metadata": {
        "id": "7L4yVSswUkMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import enable_verbose_stdout_logging\n",
        "enable_verbose_stdout_logging"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "y56QXe1jHdUS",
        "outputId": "11f75110-9be9-49fd-94c6-e981eee70a3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function agents.enable_verbose_stdout_logging()>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>agents.enable_verbose_stdout_logging</b><br/>def enable_verbose_stdout_logging()</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/agents/__init__.py</a>Enables verbose logging to stdout. This is useful for debugging.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 151);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import enable_verbose_stdout_logging\n",
        "enable_verbose_stdout_logging\n",
        "import asyncio\n",
        "from dataclasses import dataclass\n",
        "from agents import Agent, Runner, RunContextWrapper, function_tool\n",
        "\n",
        "@dataclass\n",
        "class UserInfo:\n",
        "    name: str\n",
        "    location: str\n",
        "\n",
        "@function_tool\n",
        "async def greet_user(wrapper: RunContextWrapper[UserInfo], greetings: str) -> str:\n",
        "    \"\"\"Greet user with their name.\n",
        "\n",
        "    Args:\n",
        "        greetings: A specialized greeting for user\n",
        "    \"\"\"\n",
        "    name = wrapper.context.name\n",
        "    return f\"Hello {name}, {greetings}\"\n",
        "\n",
        "# Define agent at module level\n",
        "agent = Agent[UserInfo](\n",
        "    name=\"Assistant\",\n",
        "    tools=[greet_user],\n",
        "    model=model,  # Assumes model is defined (e.g., gemini-2.0-flash)\n",
        "    instructions=\"Always greet the user using the greet_user tool and welcome them to University of Sindh.\"\n",
        ")\n",
        "\n",
        "async def main():\n",
        "    user_info = UserInfo(name=\"Soomro\", location=\"Pakistan\")\n",
        "\n",
        "    result = await Runner.run(\n",
        "        starting_agent=agent,\n",
        "        input=\"Greet the student\",  # Changed to trigger tool\n",
        "        context=user_info\n",
        "    )\n",
        "\n",
        "    print(f\"Assistant: {result.final_output}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RgBN85hPhDF",
        "outputId": "d329fcea-9d0e-4de7-c923-5615e180fa16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assistant: Hello Soomro, Welcome to University of Sindh!\n",
            "\n"
          ]
        }
      ]
    }
  ]
}